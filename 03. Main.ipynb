{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import argparse\n",
    "from packages.vocab import Vocab\n",
    "import torch.nn.functional as F\n",
    "from tensorboard.logger import Logger\n",
    "from torch.autograd import Variable\n",
    "from packages.data_loader import get_loader\n",
    "from models.extractor import JavascriptExtractor\n",
    "from packages.functions import pack_padded, to_np, to_var, str2bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    train_root='/home/irteam/users/data/D3/outputs_train.txt'\n",
    "    val_root='/home/irteam/users/data/D3/outputs_val.txt'\n",
    "    test_root='/home/irteam/users/data/D3/outputs_test.txt'\n",
    "    dict_root='data/dict_1000.json'\n",
    "    max_oovs=20\n",
    "    mode='test'\n",
    "    epochs=20\n",
    "    hidden=256\n",
    "    embed=256\n",
    "    lr=0.01\n",
    "    log=False\n",
    "    load='data/model_9000_steps.pckl'\n",
    "#     load = None\n",
    "    copy=False\n",
    "    n_layers=2\n",
    "    n_head=8\n",
    "    similarity='mlp'\n",
    "    max_in_seq=150\n",
    "    max_out_seq=150\n",
    "    batch=64\n",
    "    encoder='lstm'\n",
    "    single=False\n",
    "    cuda=True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    print(args)\n",
    "    if args.log:\n",
    "        logger = Logger('./logs')\n",
    "    vocab = Vocab(args.dict_root, args.max_oovs)\n",
    "    data_loader = get_loader(args.train_root, args.dict_root, vocab, args.batch, args.single)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    if args.load is None:\n",
    "        model = JavascriptExtractor(args,vocab)\n",
    "    else:\n",
    "        model = torch.load(args.load)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    steps = 0\n",
    "    opt = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    total_batches=0\n",
    "    for epoch in range(args.epochs):\n",
    "        within_steps = 0\n",
    "        for i, (inputs, lengths, labels, oovs) in enumerate(data_loader):\n",
    "            # split tuples\n",
    "            steps+=1\n",
    "            total_batches = max(total_batches,i)\n",
    "            model.zero_grad()\n",
    "            sources, queries, targets = inputs\n",
    "            source_len, query_len, target_len, context_len= lengths\n",
    "            if args.cuda:\n",
    "                sources = sources.cuda()\n",
    "                queries = queries.cuda()\n",
    "                targets = targets.cuda()\n",
    "            if args.single:\n",
    "                outputs = model(sources,queries,lengths, targets) # [batch x seq x vocab]\n",
    "            else:\n",
    "                outputs, sim = model(sources,queries,lengths, targets) # [batch x seq x vocab]\n",
    "            targets = Variable(targets[:,1:])\n",
    "            packed_outputs,packed_targets = pack_padded(outputs,targets)\n",
    "            packed_outputs = torch.log(packed_outputs)\n",
    "            if args.single:\n",
    "                loss = criterion(packed_outputs,packed_targets)\n",
    "            else:\n",
    "                sim = sim + 1e-3\n",
    "                sim = torch.log(sim)\n",
    "                labels = Variable(torch.LongTensor(list(labels)))\n",
    "                if args.cuda:\n",
    "                    labels = labels.cuda()\n",
    "                loss1 = criterion(sim, labels)\n",
    "                loss2 = criterion(packed_outputs,packed_targets)\n",
    "                loss = loss1 + loss2\n",
    "                # loss = loss1\n",
    "            predicted = packed_outputs.max(1)[1]\n",
    "            correct=(predicted==packed_targets).long().sum()\n",
    "            acc = (correct.data[0]*1.0/packed_targets.size(0))\n",
    "            if args.single:\n",
    "                print(\"[%d]: Epoch %d\\t%d/%d\\tLoss: %1.3f\\tAccuracy: %1.3f\"\n",
    "                      %(steps,epoch+1,i,total_batches,\n",
    "                        loss.data[0],acc))\n",
    "            else:\n",
    "                predicted_label = sim.max(1)[1]\n",
    "                correct_label=(predicted_label==labels).long().sum()\n",
    "                acc2 = (correct_label.data[0]*1.0/len(labels))\n",
    "                print(\"[%d]: Epoch %d\\t%d/%d\\tLoss: %1.3f, %1.3f\\tAccuracy: %1.3f, %1.3f\"\n",
    "                      %(steps,epoch+1,i,total_batches,\n",
    "                        loss1.data[0],loss2.data[0],acc2,acc))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if steps%100==0:\n",
    "                val(model,vocab, args) \n",
    "                torch.save(obj=model,f='data/model_%d_steps.pckl'%steps)\n",
    "                print(\"Model saved...\")\n",
    "                if args.log:\n",
    "                    # log scalar values\n",
    "                    info = {'loss': loss.data[0],\n",
    "                            'acc': acc}\n",
    "                    for tag,value in info.items():\n",
    "                        logger.scalar_summary(tag,value,steps)\n",
    "\n",
    "                    # log values and gradients of the parameters\n",
    "                    for tag, value in model.named_parameters():\n",
    "                        tag = tag.replace('.','/')\n",
    "                        logger.histo_summary(tag, to_np(value), steps)\n",
    "                        logger.histo_summary(tag+'/grad',to_np(value.grad), steps)\n",
    "                        \n",
    "def val(model, vocab, args):\n",
    "    criterion = nn.NLLLoss()\n",
    "    mode = 'Validation results:'\n",
    "    data_loader = get_loader(args.val_root, args.dict_root, vocab, args.batch, \n",
    "                          args.single, shuffle=False)\n",
    "    total_cases = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    for i, (inputs, lengths, labels, oovs) in enumerate(data_loader):\n",
    "        model.eval()\n",
    "        sources, queries, targets = inputs\n",
    "        source_len, query_len, target_len, context_len= lengths\n",
    "        if args.cuda:\n",
    "            sources = sources.cuda()\n",
    "            queries = queries.cuda()\n",
    "            targets = targets.cuda()\n",
    "        if args.single:\n",
    "            outputs = model(sources,queries,lengths, targets) # [batch x seq x vocab]\n",
    "        else:\n",
    "            outputs, sim = model(sources,queries,lengths,targets)\n",
    "        targets = Variable(targets[:,1:])\n",
    "\n",
    "        packed_outputs,packed_targets = pack_padded(outputs,targets)\n",
    "        packed_outputs = torch.log(packed_outputs)\n",
    "        if args.single:\n",
    "            loss = criterion(packed_outputs,packed_targets)\n",
    "        else:\n",
    "            sim = sim + 1e-3\n",
    "            sim = torch.log(sim)\n",
    "            labels = Variable(torch.LongTensor(list(labels)))\n",
    "            if args.cuda:\n",
    "                labels = labels.cuda()\n",
    "            loss1 = criterion(sim, labels)\n",
    "            loss2 = criterion(packed_outputs,packed_targets)\n",
    "            loss = loss1 + loss2\n",
    "            # loss = loss1\n",
    "        predicted = packed_outputs.max(1)[1]\n",
    "        correct=(predicted==packed_targets).long().sum()\n",
    "        acc = (correct.data[0]*1.0/packed_targets.size(0))\n",
    "        if args.single:\n",
    "            print(\"Loss: %1.3f\\tAccuracy: %1.3f\"\n",
    "                  %(loss.data[0],acc))\n",
    "        else:\n",
    "            predicted_label = sim.max(1)[1]\n",
    "            correct_label=(predicted_label==labels).long().sum()\n",
    "            acc2 = (correct_label.data[0]*1.0/len(labels))\n",
    "            print(\"Loss: %1.3f, %1.3f\\tAccuracy: %1.3f, %1.3f\"\n",
    "                  %(loss1.data[0],loss2.data[0],acc2,acc))\n",
    "    return\n",
    "\n",
    "def test(args):\n",
    "    vocab = Vocab(args.dict_root, args.max_oovs)\n",
    "    criterion = nn.NLLLoss()\n",
    "    if args.load is None:\n",
    "        print(\"Error: no model found\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        model = torch.load(args.load)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    total_batches=0\n",
    "    args.val_root = args.test_root # to apply val function directly\n",
    "    val(model, vocab, args)\n",
    "    return\n",
    "\n",
    "def copy(args):\n",
    "    import os\n",
    "    import datetime\n",
    "    from distutils.dir_util import copy_tree\n",
    "    folder_dir = os.path.join('data',datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    os.mkdir(folder_dir)\n",
    "    from_list = ['models/','packages/']\n",
    "    for item in from_list:\n",
    "        from_dir = item\n",
    "        to_dir = os.path.join(folder_dir,item)\n",
    "        copy_tree(from_dir, to_dir)    \n",
    "    print(\"Folders copied at %s\" %folder_dir)\n",
    "    return\n",
    "\n",
    "def main(args):\n",
    "    if args.copy==True:\n",
    "        copy(args)\n",
    "    if args.mode=='train':\n",
    "        print(\"Train mode\")\n",
    "        train(args)\n",
    "    elif args.mode=='test':\n",
    "        print(\"Test mode\")\n",
    "        test(args)\n",
    "    else:\n",
    "        print(\"Error: please specify --mode as 'train' or 'test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mode\n",
      "Loss: 0.898, 1.488\tAccuracy: 0.719, 0.736\n",
      "Loss: 1.347, 1.168\tAccuracy: 0.359, 0.795\n",
      "Loss: 1.184, 1.091\tAccuracy: 0.469, 0.796\n",
      "Loss: 1.161, 1.380\tAccuracy: 0.547, 0.736\n",
      "Loss: 1.034, 2.007\tAccuracy: 0.625, 0.730\n",
      "Loss: 1.135, 2.321\tAccuracy: 0.578, 0.546\n",
      "Loss: 1.083, 2.644\tAccuracy: 0.656, 0.583\n",
      "Loss: 0.461, 0.527\tAccuracy: 0.906, 0.884\n",
      "Loss: 0.673, 1.159\tAccuracy: 0.859, 0.827\n",
      "Loss: 0.691, 1.395\tAccuracy: 0.812, 0.799\n",
      "Loss: 0.430, 0.234\tAccuracy: 0.922, 0.926\n",
      "Loss: 0.934, 0.974\tAccuracy: 0.703, 0.820\n",
      "Loss: 0.505, 0.298\tAccuracy: 0.938, 0.879\n",
      "Loss: 0.629, 1.385\tAccuracy: 0.859, 0.770\n",
      "Loss: 0.982, 1.715\tAccuracy: 0.578, 0.698\n",
      "Loss: 0.853, 2.580\tAccuracy: 0.672, 0.581\n",
      "Loss: 0.890, 2.065\tAccuracy: 0.688, 0.638\n",
      "Loss: 0.495, 2.400\tAccuracy: 0.812, 0.641\n",
      "Loss: 0.991, 2.758\tAccuracy: 0.500, 0.547\n",
      "Loss: 0.831, 2.408\tAccuracy: 0.656, 0.589\n",
      "Loss: 0.714, 2.581\tAccuracy: 0.750, 0.537\n",
      "Loss: 1.018, 2.116\tAccuracy: 0.531, 0.620\n",
      "Loss: 1.044, 2.273\tAccuracy: 0.641, 0.608\n",
      "Loss: 1.264, 1.689\tAccuracy: 0.578, 0.762\n",
      "Loss: 1.180, 1.618\tAccuracy: 0.656, 0.752\n",
      "Loss: 1.220, 1.618\tAccuracy: 0.656, 0.745\n",
      "Loss: 1.147, 1.537\tAccuracy: 0.672, 0.770\n",
      "Loss: 1.167, 2.681\tAccuracy: 0.594, 0.613\n",
      "Loss: 1.263, 1.711\tAccuracy: 0.625, 0.758\n",
      "Loss: 1.106, 1.497\tAccuracy: 0.672, 0.791\n",
      "Loss: 1.194, 1.593\tAccuracy: 0.641, 0.753\n",
      "Loss: 1.828, 2.174\tAccuracy: 0.531, 0.600\n",
      "Loss: 1.152, 2.745\tAccuracy: 0.625, 0.605\n",
      "Loss: 1.946, 3.781\tAccuracy: 0.406, 0.492\n",
      "Loss: 1.157, 2.701\tAccuracy: 0.609, 0.588\n",
      "Loss: 1.083, 2.477\tAccuracy: 0.562, 0.615\n",
      "Loss: 0.893, 1.793\tAccuracy: 0.672, 0.703\n",
      "Loss: 0.981, 2.375\tAccuracy: 0.594, 0.619\n",
      "Loss: 1.081, 2.234\tAccuracy: 0.594, 0.694\n",
      "Loss: 0.865, 2.646\tAccuracy: 0.531, 0.558\n",
      "Loss: 1.449, 3.470\tAccuracy: 0.594, 0.469\n",
      "Loss: 1.703, 3.347\tAccuracy: 0.344, 0.507\n",
      "Loss: 1.195, 3.240\tAccuracy: 0.500, 0.509\n",
      "Loss: 1.310, 1.746\tAccuracy: 0.641, 0.768\n",
      "Loss: 1.215, 2.393\tAccuracy: 0.391, 0.646\n",
      "Loss: 0.785, 2.364\tAccuracy: 0.750, 0.620\n",
      "Loss: 1.418, 3.506\tAccuracy: 0.656, 0.449\n",
      "Loss: 0.574, 2.493\tAccuracy: 0.781, 0.530\n",
      "Loss: 1.381, 1.384\tAccuracy: 0.406, 0.765\n",
      "Loss: 0.966, 1.529\tAccuracy: 0.656, 0.687\n",
      "Loss: 1.354, 1.627\tAccuracy: 0.297, 0.614\n",
      "Loss: 1.365, 1.251\tAccuracy: 0.328, 0.725\n",
      "Loss: 1.053, 1.768\tAccuracy: 0.625, 0.679\n",
      "Loss: 1.529, 2.273\tAccuracy: 0.609, 0.614\n",
      "Loss: 1.438, 2.819\tAccuracy: 0.625, 0.538\n",
      "Loss: 1.363, 2.470\tAccuracy: 0.484, 0.632\n",
      "Loss: 1.127, 2.828\tAccuracy: 0.609, 0.598\n",
      "Loss: 0.620, 2.524\tAccuracy: 0.719, 0.563\n",
      "Loss: 0.736, 1.773\tAccuracy: 0.625, 0.704\n",
      "Loss: 0.589, 2.286\tAccuracy: 0.828, 0.613\n",
      "Loss: 1.429, 3.031\tAccuracy: 0.516, 0.548\n",
      "Loss: 1.105, 2.943\tAccuracy: 0.578, 0.581\n",
      "Loss: 0.394, 2.668\tAccuracy: 0.812, 0.589\n",
      "Loss: 0.932, 2.543\tAccuracy: 0.516, 0.548\n",
      "Loss: 0.962, 2.083\tAccuracy: 0.641, 0.590\n",
      "Loss: 0.964, 2.945\tAccuracy: 0.641, 0.556\n",
      "Loss: 0.859, 2.695\tAccuracy: 0.578, 0.638\n",
      "Loss: 0.973, 2.966\tAccuracy: 0.641, 0.565\n",
      "Loss: 0.762, 2.334\tAccuracy: 0.734, 0.632\n",
      "Loss: 1.078, 2.922\tAccuracy: 0.703, 0.556\n",
      "Loss: 1.321, 2.979\tAccuracy: 0.562, 0.592\n",
      "Loss: 0.807, 3.211\tAccuracy: 0.672, 0.537\n",
      "Loss: 1.182, 3.633\tAccuracy: 0.578, 0.494\n",
      "Loss: 1.763, 3.674\tAccuracy: 0.516, 0.441\n",
      "Loss: 1.132, 2.242\tAccuracy: 0.641, 0.649\n",
      "Loss: 1.111, 2.810\tAccuracy: 0.641, 0.591\n",
      "Loss: 1.417, 2.397\tAccuracy: 0.438, 0.655\n",
      "Loss: 0.744, 2.664\tAccuracy: 0.734, 0.561\n",
      "Loss: 0.740, 3.290\tAccuracy: 0.703, 0.505\n",
      "Loss: 1.031, 2.986\tAccuracy: 0.625, 0.518\n",
      "Loss: 0.902, 2.551\tAccuracy: 0.562, 0.633\n",
      "Loss: 0.743, 2.159\tAccuracy: 0.719, 0.642\n",
      "Loss: 1.159, 2.419\tAccuracy: 0.594, 0.547\n",
      "Loss: 0.547, 1.807\tAccuracy: 0.859, 0.697\n",
      "Loss: 1.393, 3.473\tAccuracy: 0.328, 0.498\n",
      "Loss: 0.600, 1.327\tAccuracy: 0.797, 0.736\n",
      "Loss: 1.402, 2.349\tAccuracy: 0.594, 0.664\n",
      "Loss: 1.670, 3.998\tAccuracy: 0.469, 0.424\n",
      "Loss: 1.200, 2.790\tAccuracy: 0.594, 0.550\n",
      "Loss: 1.305, 2.265\tAccuracy: 0.438, 0.653\n",
      "Loss: 0.966, 2.215\tAccuracy: 0.641, 0.655\n",
      "Loss: 0.832, 2.507\tAccuracy: 0.672, 0.598\n",
      "Loss: 0.532, 2.567\tAccuracy: 0.844, 0.640\n",
      "Loss: 0.728, 2.825\tAccuracy: 0.812, 0.547\n",
      "Loss: 0.822, 2.021\tAccuracy: 0.688, 0.644\n",
      "Loss: 0.844, 2.430\tAccuracy: 0.734, 0.599\n",
      "Loss: 0.747, 3.265\tAccuracy: 0.750, 0.480\n",
      "Loss: 0.563, 2.865\tAccuracy: 1.000, 0.500\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
